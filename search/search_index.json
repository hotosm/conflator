{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OSM Merge","text":"<p> Merge features and tags into existing OSM data. </p> <p> </p> <p>\ud83d\udcd6 Documentation: https://hotosm.github.io/osm-merge/</p> <p>\ud83d\udda5\ufe0f Source Code: https://github.com/hotosm/osm-merge</p>"},{"location":"#background","title":"Background","text":"<p>This is a project for conflating map data, with the ultimate goal of importing it into OpenStreetMap. It is oriented towards processing non OSM external datasets.</p>"},{"location":"#programs","title":"Programs","text":""},{"location":"#conflatebuildingspy","title":"conflateBuildings.py","text":"<p>This looks for duplicate buildings both in the external dataset, and in OSM. This has been used with multiple datasets, namely the Microsoft ML Building footprints, Overture, and others.</p>"},{"location":"#conflatepoipy","title":"conflatePOI.py","text":"<p>This looks to find a building when the data is only a POI. Many external datasets are a list of buildings, like schools or hospitals. In OSM the only metadata for the feature may be building=yes. Also field collected data with ODK Collect is also a POI, and we want the data that was collected to be merged with any existing OSM features.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"LICENSE/","title":"GNU AFFERO GENERAL PUBLIC LICENSE","text":"<p>Version 3, 19 November 2007</p> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/</p> <p>Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p>"},{"location":"LICENSE/#preamble","title":"Preamble","text":"<p>The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.</p> <p>When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.</p> <p>A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.</p> <p>The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.</p> <p>An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p>"},{"location":"LICENSE/#terms-and-conditions","title":"TERMS AND CONDITIONS","text":""},{"location":"LICENSE/#0-definitions","title":"0. Definitions.","text":"<p>\"This License\" refers to version 3 of the GNU Affero General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p>"},{"location":"LICENSE/#1-source-code","title":"1. Source Code.","text":"<p>The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p>"},{"location":"LICENSE/#2-basic-permissions","title":"2. Basic Permissions.","text":"<p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.</p>"},{"location":"LICENSE/#3-protecting-users-legal-rights-from-anti-circumvention-law","title":"3. Protecting Users' Legal Rights From Anti-Circumvention Law.","text":"<p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p>"},{"location":"LICENSE/#4-conveying-verbatim-copies","title":"4. Conveying Verbatim Copies.","text":"<p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p>"},{"location":"LICENSE/#5-conveying-modified-source-versions","title":"5. Conveying Modified Source Versions.","text":"<p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <ul> <li>a) The work must carry prominent notices stating that you modified     it, and giving a relevant date.</li> <li>b) The work must carry prominent notices stating that it is     released under this License and any conditions added under     section 7. This requirement modifies the requirement in section 4     to \"keep intact all notices\".</li> <li>c) You must license the entire work, as a whole, under this     License to anyone who comes into possession of a copy. This     License will therefore apply, along with any applicable section 7     additional terms, to the whole of the work, and all its parts,     regardless of how they are packaged. This License gives no     permission to license the work in any other way, but it does not     invalidate such permission if you have separately received it.</li> <li>d) If the work has interactive user interfaces, each must display     Appropriate Legal Notices; however, if the Program has interactive     interfaces that do not display Appropriate Legal Notices, your     work need not make them do so.</li> </ul> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p>"},{"location":"LICENSE/#6-conveying-non-source-forms","title":"6. Conveying Non-Source Forms.","text":"<p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <ul> <li>a) Convey the object code in, or embodied in, a physical product     (including a physical distribution medium), accompanied by the     Corresponding Source fixed on a durable physical medium     customarily used for software interchange.</li> <li>b) Convey the object code in, or embodied in, a physical product     (including a physical distribution medium), accompanied by a     written offer, valid for at least three years and valid for as     long as you offer spare parts or customer support for that product     model, to give anyone who possesses the object code either (1) a     copy of the Corresponding Source for all the software in the     product that is covered by this License, on a durable physical     medium customarily used for software interchange, for a price no     more than your reasonable cost of physically performing this     conveying of source, or (2) access to copy the Corresponding     Source from a network server at no charge.</li> <li>c) Convey individual copies of the object code with a copy of the     written offer to provide the Corresponding Source. This     alternative is allowed only occasionally and noncommercially, and     only if you received the object code with such an offer, in accord     with subsection 6b.</li> <li>d) Convey the object code by offering access from a designated     place (gratis or for a charge), and offer equivalent access to the     Corresponding Source in the same way through the same place at no     further charge. You need not require recipients to copy the     Corresponding Source along with the object code. If the place to     copy the object code is a network server, the Corresponding Source     may be on a different server (operated by you or a third party)     that supports equivalent copying facilities, provided you maintain     clear directions next to the object code saying where to find the     Corresponding Source. Regardless of what server hosts the     Corresponding Source, you remain obligated to ensure that it is     available for as long as needed to satisfy these requirements.</li> <li>e) Convey the object code using peer-to-peer transmission,     provided you inform other peers where the object code and     Corresponding Source of the work are being offered to the general     public at no charge under subsection 6d.</li> </ul> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p>"},{"location":"LICENSE/#7-additional-terms","title":"7. Additional Terms.","text":"<p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <ul> <li>a) Disclaiming warranty or limiting liability differently from the     terms of sections 15 and 16 of this License; or</li> <li>b) Requiring preservation of specified reasonable legal notices or     author attributions in that material or in the Appropriate Legal     Notices displayed by works containing it; or</li> <li>c) Prohibiting misrepresentation of the origin of that material,     or requiring that modified versions of such material be marked in     reasonable ways as different from the original version; or</li> <li>d) Limiting the use for publicity purposes of names of licensors     or authors of the material; or</li> <li>e) Declining to grant rights under trademark law for use of some     trade names, trademarks, or service marks; or</li> <li>f) Requiring indemnification of licensors and authors of that     material by anyone who conveys the material (or modified versions     of it) with contractual assumptions of liability to the recipient,     for any liability that these contractual assumptions directly     impose on those licensors and authors.</li> </ul> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p>"},{"location":"LICENSE/#8-termination","title":"8. Termination.","text":"<p>You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p>"},{"location":"LICENSE/#9-acceptance-not-required-for-having-copies","title":"9. Acceptance Not Required for Having Copies.","text":"<p>You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p>"},{"location":"LICENSE/#10-automatic-licensing-of-downstream-recipients","title":"10. Automatic Licensing of Downstream Recipients.","text":"<p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p>"},{"location":"LICENSE/#11-patents","title":"11. Patents.","text":"<p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p>"},{"location":"LICENSE/#12-no-surrender-of-others-freedom","title":"12. No Surrender of Others' Freedom.","text":"<p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p>"},{"location":"LICENSE/#13-remote-network-interaction-use-with-the-gnu-general-public-license","title":"13. Remote Network Interaction; Use with the GNU General Public License.","text":"<p>Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.</p> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.</p>"},{"location":"LICENSE/#14-revised-versions-of-this-license","title":"14. Revised Versions of this License.","text":"<p>The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p>"},{"location":"LICENSE/#15-disclaimer-of-warranty","title":"15. Disclaimer of Warranty.","text":"<p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p>"},{"location":"LICENSE/#16-limitation-of-liability","title":"16. Limitation of Liability.","text":"<p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p>"},{"location":"LICENSE/#17-interpretation-of-sections-15-and-16","title":"17. Interpretation of Sections 15 and 16.","text":"<p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <p>END OF TERMS AND CONDITIONS</p>"},{"location":"LICENSE/#how-to-apply-these-terms-to-your-new-programs","title":"How to Apply These Terms to Your New Programs","text":"<p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as\n    published by the Free Software Foundation, either version 3 of the\n    License, or (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.</p>"},{"location":"about/","title":"Conflator","text":"<p>This is a project for conflating map data, with the ultimate goal of importing it into OpenStreetMap(OSM).</p> <p>It is oriented towards conflating external datasets with existing OSM data. External data is usually polygons (building footprints), or POIs. These days there are multiple publically available building footprint datasets with an appropriate license for OSM. The problem is this data needs to be validated.</p> <p>Due to the flexibility of the OSM data schema, it's impossible to get 100% perfect conflation. But the purely manual conflation is very time consuming and tedious. This project aims to do as much as possible to aid the validator to make their work as efficient as possible.</p>"},{"location":"api/","title":"API Docs for conflator","text":""},{"location":"api/#conflatorpy","title":"conflator.py","text":"<p>             Bases: <code>object</code></p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>URI for the primary database</p> <code>None</code> <code>boundary</code> <code>str</code> <p>Boundary to limit SQL queries</p> <code>None</code> <p>Returns:</p> Type Description <code>Conflator</code> <p>An instance of this object</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>def __init__(self,\n             uri: str = None,\n             boundary: str = None\n             ):\n    \"\"\"\n    Initialize Input data sources.\n\n    Args:\n        uri (str): URI for the primary database\n        boundary (str, optional): Boundary to limit SQL queries\n\n    Returns:\n        (Conflator): An instance of this object\n    \"\"\"\n    self.postgres = list()\n    self.tags = dict()\n    self.boundary = boundary\n    self.dburi = uri\n    self.primary = None\n    if boundary:\n        infile = open(boundary, 'r')\n        self.boundary = geojson.load(infile)\n        infile.close()\n    # Distance in meters for conflating with postgis\n    self.tolerance = 7\n    self.data = dict()\n    self.analyze = (\"building\", \"name\", \"amenity\", \"landuse\", \"cuisine\", \"tourism\", \"leisure\")\n</code></pre> <p>options: show_source: false heading_level: 3</p>"},{"location":"api/#osm_merge.conflator.Conflator.getDistance","title":"getDistance","text":"<pre><code>getDistance(newdata, olddata)\n</code></pre> <p>Compute the distance between two features in meters</p> <p>Parameters:</p> Name Type Description Default <code>newdata</code> <code>Feature</code> <p>A feature from the external dataset</p> required <code>olddata</code> <code>Feature</code> <p>A feature from the existing OSM dataset</p> required <p>Returns:</p> Type Description <code>float</code> <p>The distance between the two features</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>def getDistance(self,\n        newdata: Feature,\n        olddata: Feature,\n        ) -&gt; float:\n    \"\"\"\n    Compute the distance between two features in meters\n\n    Args:\n        newdata (Feature): A feature from the external dataset\n        olddata (Feature): A feature from the existing OSM dataset\n\n    Returns:\n        (float): The distance between the two features\n    \"\"\"\n    # dist = shapely.hausdorff_distance(center, wkt)\n    dist = 0.0\n\n    # Transform so the results are in meters instead of degress of the\n    # earth's radius.\n    project = pyproj.Transformer.from_proj(\n        pyproj.Proj(init='epsg:4326'),\n        pyproj.Proj(init='epsg:32633')\n        )\n    newobj = transform(project.transform, shape(newdata[\"geometry\"]))\n    oldobj = transform(project.transform, shape(olddata[\"geometry\"]))\n\n    if oldobj.geom_type == \"LineString\" and newobj.geom_type == \"LineString\":\n        # Compare two highways\n        dist = newobj.distance(oldobj)\n    elif oldobj.geom_type == \"Point\" and newobj.geom_type == \"LineString\":\n        # We only want to compare LineStrings, so force the distance check\n        # to be False\n        dist = 12345678.9\n    elif oldobj.geom_type == \"Point\" and newobj.geom_type == \"Point\":\n        dist = newobj.distance(oldobj)\n    elif oldobj.geom_type == \"Polygon\" and newobj.geom_type == \"Polygon\":\n        # compare two buildings\n        pass\n    elif oldobj.geom_type == \"Polygon\" and newobj.geom_type == \"Point\":\n        # Compare a point with a building, used for ODK Collect data\n        center = shapely.centroid(oldobj)\n        dist = newdata.distance(center)\n    elif oldobj.geom_type == \"Point\" and newobj.geom_type == \"LineString\":\n        dist = newdata.distance(oldobj)\n\n    return dist # * 111195\n</code></pre>"},{"location":"api/#osm_merge.conflator.Conflator.checkTags","title":"checkTags","text":"<pre><code>checkTags(extfeat, osm)\n</code></pre> <p>Check tags between 2 features.</p> <p>Parameters:</p> Name Type Description Default <code>extfeat</code> <code>Feature</code> <p>The feature from the external dataset</p> required <code>osm</code> <code>Feature</code> <p>The result of the SQL query</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of tag matches</p> <code>dict</code> <p>The updated tags</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>def checkTags(self,\n              extfeat: Feature,\n              osm: Feature,\n               ):\n    \"\"\"\n    Check tags between 2 features.\n\n    Args:\n        extfeat (Feature): The feature from the external dataset\n        osm (Feature): The result of the SQL query\n\n    Returns:\n        (int): The number of tag matches\n        (dict): The updated tags\n    \"\"\"\n    match_threshold = 80\n    hits = 0\n    props = dict()\n    id = 0\n    version = 0\n    for key, value in extfeat['properties'].items():\n        if key in osm[\"properties\"]:\n            if key == \"title\" or key == \"label\":\n                # ODK data extracts have an title and image tags,\n                # which is usually just a duplicate of the name,\n                # so drop those.\n                continue\n            elif key == \"osm_id\" or key == \"id\":\n                # External data not from an OSM source always has\n                # negative IDs to distinguish it from current OSM data.\n                if value &lt;= 0:\n                    id = int(osm[\"properties\"][key])\n                else:\n                    id = int(value)\n                props[\"id\"] = id\n                continue\n            elif key == \"version\":\n                # Always use the OSM version, since it gets incremented\n                # so JOSM see it's been modified.\n                version = int(osm[\"properties\"][key])\n                props[\"version\"] = version\n                continue\n            # Name may also be name:en, name:np, etc... There may also be\n            # multiple name:* values in the tags.\n            elif key[:4] == \"name\":\n                # Usually it's the name field that has the most variety in\n                # in trying to match strings. This often is differences in\n                # capitalization, singular vs plural, and typos from using\n                # your phone to enter the name. Course names also change\n                # too so if it isn't a match, use the new name from the\n                # external dataset.\n                ratio = fuzz.ratio(value.lower(), osm[\"properties\"][key].lower())\n                if ratio &gt; match_threshold:\n                    hits += 1\n                    props[\"ratio\"] = ratio\n                    props[key] = value\n                    if value != osm[\"properties\"][key]:\n                        props[f\"old_{key}\"] = osm[\"properties\"][key]\n                else:\n                    if key != 'note':\n                        props[key] = value\n            else:\n                # All the other keys are usually a defined OSM tag.\n                # Course the new value is probably more up to data\n                # than what is in OSM. Keep both in the properties\n                # for debugging tag conflation.\n                if key == \"title\" or key == \"label\":\n                    continue\n                props[key] = value\n                if value != osm[\"properties\"][key]:\n                    props[f\"old_{key}\"] = osm[\"properties\"][key]\n                else:\n                    hits += 1\n        else:\n            # Add the tag from the new data since it isn't in OSM yet.\n            props[key] = value\n\n    return hits, props\n</code></pre>"},{"location":"api/#osm_merge.conflator.Conflator.loadFile","title":"loadFile","text":"<pre><code>loadFile(osmfile)\n</code></pre> <p>Read a OSM XML file generated by osm_fieldwork and convert it to GeoJson for consistency.</p> <p>Parameters:</p> Name Type Description Default <code>osmfile</code> <code>str</code> <p>The OSM XML file to load</p> required <p>Returns:</p> Type Description <code>list</code> <p>The entries in the OSM XML file</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>def loadFile(\n    self,\n    osmfile: str,\n) -&gt; list:\n    \"\"\"\n    Read a OSM XML file generated by osm_fieldwork and convert\n    it to GeoJson for consistency.\n\n    Args:\n        osmfile (str): The OSM XML file to load\n\n    Returns:\n        (list): The entries in the OSM XML file\n    \"\"\"\n    alldata = list()\n    size = os.path.getsize(osmfile)\n    with open(osmfile, \"r\") as file:\n        xml = file.read(size)\n        doc = xmltodict.parse(xml)\n        if \"osm\" not in doc:\n            logging.warning(\"No data in this instance\")\n            return False\n        data = doc[\"osm\"]\n        if \"node\" not in data:\n            logging.warning(\"No nodes in this instance\")\n            return False\n\n    nodes = dict()\n    for node in data[\"node\"]:\n        properties = {\n            \"id\": int(node[\"@id\"]),\n        }\n        if \"@version\" not in node:\n            properties[\"version\"] = 1\n        else:\n            properties[\"version\"] = node[\"@version\"]\n\n        if \"@timestamp\" in node:\n            properties[\"timestamp\"] = node[\"@timestamp\"]\n\n        if \"tag\" in node:\n            for tag in node[\"tag\"]:\n                if type(tag) == dict:\n                    properties[tag[\"@k\"]] = tag[\"@v\"].strip()\n                    # continue\n                else:\n                    properties[node[\"tag\"][\"@k\"]] = node[\"tag\"][\"@v\"].strip()\n                # continue\n        geom = Point((float(node[\"@lon\"]), float(node[\"@lat\"])))\n        # cache the nodes so we can dereference the refs into\n        # coordinates, but we don't need them in GeoJson format.\n        nodes[properties[\"id\"]] = geom\n        if len(properties) &gt; 2:\n            alldata.append(Feature(geometry=geom, properties=properties))\n\n    for way in data[\"way\"]:\n        properties = {\n            \"id\": int(way[\"@id\"]),\n        }\n        refs = list()\n        if len(way[\"nd\"]) &gt; 0:\n            for ref in way[\"nd\"]:\n                refs.append(int(ref[\"@ref\"]))\n        properties[\"refs\"] = refs\n\n        if \"@version\" not in node:\n            properties[\"version\"] = 1\n        else:\n            properties[\"version\"] = node[\"@version\"]\n\n        if \"@timestamp\" in node:\n            attrs[\"timestamp\"] = node[\"@timestamp\"]\n\n        if \"tag\" in way:\n            for tag in way[\"tag\"]:\n                if type(tag) == dict:\n                    properties[tag[\"@k\"]] = tag[\"@v\"].strip()\n                    # continue\n                else:\n                    properties[way[\"tag\"][\"@k\"]] = way[\"tag\"][\"@v\"].strip()\n                # continue\n        # geom =\n        tmp = list()\n        for ref in refs:\n            tmp.append(nodes[ref]['coordinates'])\n        geom = LineString(tmp)\n        alldata.append(Feature(geometry=geom, properties=properties))\n\n    return alldata\n</code></pre>"},{"location":"api/#osm_merge.conflator.Conflator.initInputDB","title":"initInputDB  <code>async</code>","text":"<pre><code>initInputDB(config=None, dburi=None)\n</code></pre> <p>When async, we can't initialize the async database connection, so it has to be done as an extrat step.</p> <p>Parameters:</p> Name Type Description Default <code>dburi</code> <code>str</code> <p>The database URI</p> <code>None</code> <code>config</code> <code>str</code> <p>The config file from the osm-rawdata project</p> <code>None</code> <p>Returns:     (bool): Whether it initialiized</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>async def initInputDB(self,\n                    config: str = None,\n                    dburi: str = None,\n                    ) -&gt; bool:\n    \"\"\"\n    When async, we can't initialize the async database connection,\n    so it has to be done as an extrat step.\n\n    Args:\n        dburi (str, optional): The database URI\n        config (str, optional): The config file from the osm-rawdata project\n    Returns:\n        (bool): Whether it initialiized\n    \"\"\"\n    db = GeoSupport(dburi, config)\n    await db.initialize()\n    self.postgres.append(db)\n\n    return True\n</code></pre>"},{"location":"api/#osm_merge.conflator.Conflator.initOutputDB","title":"initOutputDB  <code>async</code>","text":"<pre><code>initOutputDB(dburi=None)\n</code></pre> <p>When async, we can't initialize the async database connection, so it has to be done as an extrat step.</p> <p>Parameters:</p> Name Type Description Default <code>dburi</code> <code>str</code> <p>The database URI</p> <code>None</code> <code>config</code> <code>str</code> <p>The config file from the osm-rawdata project</p> required Source code in <code>osm_merge/conflator.py</code> <pre><code>async def initOutputDB(self,\n                    dburi: str = None,\n                    ):\n    \"\"\"\n    When async, we can't initialize the async database connection,\n    so it has to be done as an extrat step.\n\n    Args:\n        dburi (str, optional): The database URI\n        config (str, optional): The config file from the osm-rawdata project\n    \"\"\"\n    if dburi:\n        self.dburi = dburi\n        await self.createDBThreads(dburi, config)\n    elif self.dburi:\n        await self.createDBThreads(self.dburi, config)\n</code></pre>"},{"location":"api/#osm_merge.conflator.Conflator.createDBThreads","title":"createDBThreads  <code>async</code>","text":"<pre><code>createDBThreads(uri=None, config=None, execs=cores)\n</code></pre> <p>Create threads for writting to the primary datatbase to avoid problems with corrupting data.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>URI for the primary database</p> <code>None</code> <code>config</code> <code>str</code> <p>The config file from the osm-rawdata project</p> <code>None</code> <code>threads</code> <code>int</code> <p>The number of threads to create</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the threads were created sucessfully</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>async def createDBThreads(self,\n                    uri: str = None,\n                    config: str = None,\n                    execs: int = cores,\n                    ) -&gt; bool:\n    \"\"\"\n    Create threads for writting to the primary datatbase to avoid\n    problems with corrupting data.\n\n    Args:\n        uri (str): URI for the primary database\n        config (str, optional): The config file from the osm-rawdata project\n        threads (int, optional): The number of threads to create\n\n    Returns:\n        (bool): Whether the threads were created sucessfully\n    \"\"\"\n    # Each thread needs it's own connection to postgres to avoid problems\n    # when inserting or updating the primary database.\n    if uri:\n        for thread in range(0, execs + 1):\n            db = GeoSupport(uri)\n            await db.initialize(uri, config)\n            if not db:\n                return False\n            self.postgres.append(db)\n        if self.boundary:\n            if 'features' in self.boundary:\n                poly = self.boundary[\"features\"][0][\"geometry\"]\n            else:\n                poly = shape(self.boundary['geometry'])\n\n            # FIXME: we only need to clip once to create the view, this is not\n            # confirmed yet.\n            await db.clipDB(poly, self.postgres[0])\n\n        return True\n</code></pre>"},{"location":"api/#osm_merge.conflator.Conflator.conflateData","title":"conflateData  <code>async</code>","text":"<pre><code>conflateData(odkspec, osmspec, threshold=10.0)\n</code></pre> <p>Open the two source files and contlate them.</p> <p>Parameters:</p> Name Type Description Default <code>odkspec</code> <code>str</code> <p>The external data uri</p> required <code>osmspec</code> <code>str</code> <p>The existing OSM data uri</p> required <code>threshold</code> <code>float</code> <p>Threshold for distance calculations in meters</p> <code>10.0</code> <p>Returns:</p> Type Description <code>list</code> <p>The conflated output</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>async def conflateData(self,\n                odkspec: str,\n                osmspec: str,\n                threshold: float = 10.0,\n                ) -&gt; list:\n    \"\"\"\n    Open the two source files and contlate them.\n\n    Args:\n        odkspec (str): The external data uri\n        osmspec (str): The existing OSM data uri\n        threshold (float): Threshold for distance calculations in meters\n\n    Returns:\n        (list):  The conflated output\n    \"\"\"\n    timer = Timer(text=\"conflateData() took {seconds:.0f}s\")\n    timer.start()\n    odkdata = list()\n    osmdata = list()\n\n    result = list()\n    if odkspec[:3].lower() == \"pg:\":\n        db = GeoSupport(odkspec[3:])\n        result = await db.queryDB()\n    else:\n        odkdata = self.parseFile(odkspec)\n\n    if osmspec[:3].lower() == \"pg:\":\n        db = GeoSupport(osmspec[3:])\n        result = await db.queryDB()\n    else:\n        osmdata = self.parseFile(osmspec)\n\n    entries = len(odkdata)\n    chunk = round(entries / cores)\n\n    alldata = list()\n    tasks = list()\n\n    # Make threading optional for easier debugging\n    single = False\n    if single:\n        conflateThread(odkdata, osmdata)\n    else:\n        futures = list()\n        with concurrent.futures.ProcessPoolExecutor(max_workers=cores) as executor:\n            for block in range(0, entries, chunk):\n                future = executor.submit(conflateThread,\n                        odkdata[block:block + chunk - 1],\n                        osmdata\n                        )\n                futures.append(future)\n            #for thread in concurrent.futures.wait(futures, return_when='ALL_COMPLETED'):\n            for future in concurrent.futures.as_completed(futures):\n                log.debug(f\"Waiting for thread to complete.. {future.result()}\")\n                alldata += future.result()\n\n        executor.shutdown()\n\n    # async with asyncio.TaskGroup() as tg:\n    #     for block in range(0, entries, chunk):\n    #         log.debug(f\"Dispatching thread {block}:{block + chunk}\")\n    #         # task = tg.create_task(conflateThread(odkdata[block:block + chunk - 1], osmdata))\n    #         # tasks.append(task)\n    #         tasks.append(tg.create_task(conflateThread(odkdata[block:block + chunk - 1], osmdata)))\n    timer.stop()\n\n    # return await conflateThread(odkdata, osmdata, threshold)\n    return alldata\n</code></pre>"},{"location":"api/#osm_merge.conflator.Conflator.dump","title":"dump","text":"<pre><code>dump()\n</code></pre> <p>Dump internal data</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>def dump(self):\n    \"\"\"Dump internal data\"\"\"\n    print(f\"Data source is: {self.dburi}\")\n    print(f\"There are {len(self.data)} existing features\")\n</code></pre>"},{"location":"api/#osm_merge.conflator.Conflator.conflateDB","title":"conflateDB","text":"<pre><code>conflateDB(source)\n</code></pre> <p>Conflate all the data. This the primary interfacte for conflation.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source file to conflate</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The conflated features</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>def conflateDB(self,\n                 source: str,\n                 ) -&gt; dict:\n    \"\"\"\n    Conflate all the data. This the primary interfacte for conflation.\n\n    Args:\n        source (str): The source file to conflate\n\n    Returns:\n        (dict):  The conflated features\n    \"\"\"\n    timer = Timer(text=\"conflateData() took {seconds:.0f}s\")\n    timer.start()\n\n    log.info(\"Opening data file: %s\" % source)\n    toplevel = Path(source)\n    if toplevel.suffix == \".geosjon\":\n        src = open(source, \"r\")\n        self.data = geojson.load(src)\n    elif toplevel.suffix == \".osm\":\n        src = open(source, \"r\")\n        osmin = OsmFile()\n        self.data = osmin.loadFile(source) # input file\n        if self.boundary:\n            gs = GeoSupport(source)\n            # self.data = gs.clipFile(self.data)\n\n    # Use fuzzy string matching to handle minor issues in the name column,\n    # which is often used to match an amenity.\n    if len(self.data) == 0:\n        self.postgres[0].query(\"CREATE EXTENSION IF NOT EXISTS fuzzystrmatch\")\n    # log.debug(f\"OdkMerge::conflateData() called! {len(odkdata)} features\")\n\n    # A chunk is a group of threads\n    chunk = round(len(self.data) / cores)\n\n    # cycle = range(0, len(odkdata), chunk)\n\n    # Chop the data into a subset for each thread\n    newdata = list()\n    future = None\n    result = None\n    index = 0\n    if True:                # DEBUGGING HACK ALERT!\n        result = conflateThread(self.data, self, index)\n        return dict()\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cores) as executor:\n        i = 0\n        subset = dict()\n        futures = list()\n        for key, value in self.data.items():\n            subset[key] = value\n            if i == chunk:\n                i = 0\n                result = executor.submit(conflateThread, subset, self, index)\n                index += 1\n                # result.add_done_callback(callback)\n                futures.append(result)\n                subset = dict()\n            i += 1\n        for future in concurrent.futures.as_completed(futures):\n        # # for future in concurrent.futures.wait(futures, return_when='ALL_COMPLETED'):\n            log.debug(f\"Waiting for thread to complete..\")\n            # print(f\"YYEESS!! {future.result(timeout=10)}\")\n            newdata.append(future.result(timeout=5))\n    timer.stop()\n    return newdata\n</code></pre>"},{"location":"api/#osm_merge.conflator.Conflator.writeOSM","title":"writeOSM","text":"<pre><code>writeOSM(data, filespec)\n</code></pre> <p>Write the data to an OSM XML file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The list of GeoJson features</p> required <code>filespec</code> <code>str</code> <p>The output file name</p> required Source code in <code>osm_merge/conflator.py</code> <pre><code>def writeOSM(self,\n             data: dict,\n             filespec: str,\n             ):\n    \"\"\"\n    Write the data to an OSM XML file.\n\n    Args:\n        data (dict): The list of GeoJson features\n        filespec (str): The output file name\n    \"\"\"\n    osm = OsmFile(filespec)\n    for entry in data:\n        id = -1\n        if \"osm_id\" in entry[\"properties\"]:\n            id = entry[\"properties\"][\"osm_id\"]\n        elif \"id\" in entry[\"properties\"]:\n            id = entry[\"properties\"][\"id\"]\n        try:\n            attrs = {\"id\": id, \"version\": entry[\"properties\"][\"version\"]}\n        except:\n            breakpoint()\n        tags = entry[\"properties\"]\n        # These are OSM attributes, not tags\n        if \"id\" in tags:\n            del tags[\"id\"]\n        if \"version\" in tags:\n            del tags[\"version\"]\n        item = {\"attrs\": attrs, \"tags\": tags}\n        # if entry[\"geometry\"][\"type\"] == \"LineString\" or entry[\"geometry\"][\"type\"] == \"Polygon\":\n        print(entry)\n        if 'refs' in tags:\n        # if \"geometry\" not in entry:\n            # OSM ways don't have a geometry, just references to node IDs.\n            # The OSM XML file won't have any nodes, so at first won't\n            # display in JOSM until you do a File-&gt;\"Update modified\",\n            if len(tags['refs']) &gt; 0:\n                if type(tags[\"refs\"]) != list:\n                    item[\"refs\"] = eval(tags[\"refs\"])\n                else:\n                    item[\"refs\"] = tags[\"refs\"]\n                del tags[\"refs\"]\n                out = osm.createWay(item, True)\n        else:\n            out = osm.createNode(item, True)\n        if len(out) &gt; 0:\n            osm.write(out)\n</code></pre>"},{"location":"api/#osm_merge.conflator.Conflator.writeGeoJson","title":"writeGeoJson","text":"<pre><code>writeGeoJson(data, filespec)\n</code></pre> <p>Write the data to a GeoJson file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The list of GeoJson features</p> required <code>filespec</code> <code>str</code> <p>The output file name</p> required Source code in <code>osm_merge/conflator.py</code> <pre><code>def writeGeoJson(self,\n             data: dict,\n             filespec: str,\n             ):\n    \"\"\"\n    Write the data to a GeoJson file.\n\n    Args:\n        data (dict): The list of GeoJson features\n        filespec (str): The output file name\n    \"\"\"\n    file = open(filespec, \"w\")\n    fc = FeatureCollection(data)\n    geojson.dump(fc, file)\n</code></pre>"},{"location":"api/#osm_merge.conflator.Conflator.osmToFeature","title":"osmToFeature","text":"<pre><code>osmToFeature(osm)\n</code></pre> <p>Convert an entry from an OSM XML file with attrs and tags into a GeoJson Feature.</p> <p>Parameters:</p> Name Type Description Default <code>osm</code> <code>dict</code> <p>The OSM entry</p> required <p>Returns:</p> Type Description <code>Feature</code> <p>A GeoJson feature</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>def osmToFeature(self,\n                 osm: dict(),\n                 ) -&gt; Feature:\n    \"\"\"\n    Convert an entry from an OSM XML file with attrs and tags into\n    a GeoJson Feature.\n\n    Args:\n        osm (dict): The OSM entry\n\n    Returns:\n        (Feature): A GeoJson feature\n    \"\"\"\n    if \"attrs\" not in osm:\n        return Feature(geometry=shape(osm[\"geometry\"]), properties=osm[\"properties\"])\n\n    if \"osm_id\" in osm[\"attrs\"]:\n        id = osm[\"attrs\"][\"osm_id\"]\n    elif \"id\" in osm[\"attrs\"]:\n        id = osm[\"attrs\"][\"id\"]\n    props = {\"id\": id}\n    if \"version\" in osm[\"attrs\"]:\n        props[\"version\"] = osm[\"attrs\"][\"version\"]\n\n    props.update(osm[\"tags\"])\n    # It's a way, so no coordinate\n    if \"refs\" in osm:\n        return Feature(properties=props)\n    else:\n        geom = Point((float(osm[\"attrs\"][\"lon\"]), float(osm[\"attrs\"][\"lat\"])))\n\n        return Feature(geometry=geom, properties=props)\n</code></pre>"},{"location":"api/#conflatebuildingspy","title":"conflateBuildings.py","text":"<p>             Bases: <code>object</code></p> <p>Parameters:</p> Name Type Description Default <code>dburi</code> <code>str</code> <p>The DB URI</p> <code>None</code> <code>boundary</code> <code>Polygon</code> <p>The AOI of the project</p> <code>None</code> <p>Returns:</p> Type Description <code>ConflateDB</code> <p>An instance of this object</p> Source code in <code>osm_merge/conflateBuildings.py</code> <pre><code>def __init__(\n    self,\n    dburi: str = None,\n    boundary: Polygon = None,\n):\n    \"\"\"This class conflates data that has been imported into a postgres\n    database using the Underpass raw data schema.\n\n    Args:\n        dburi (str): The DB URI\n        boundary (Polygon): The AOI of the project\n\n    Returns:\n        (ConflateDB): An instance of this object\n    \"\"\"\n    self.postgres = list()\n    self.uri = None\n    if dburi:\n        self.uri = uriParser(dburi)\n        self.db = GeoSupport(dburi)\n    self.boundary = boundary\n    self.view = \"ways_poly\"\n    self.filter = list()\n</code></pre> <p>options: show_source: false heading_level: 3</p>"},{"location":"api/#osm_merge.conflateBuildings.ConflateBuildings.addSourceFilter","title":"addSourceFilter","text":"<pre><code>addSourceFilter(source)\n</code></pre> <p>Add to a list of suspect bad source datasets</p> Source code in <code>osm_merge/conflateBuildings.py</code> <pre><code>def addSourceFilter(\n    self,\n    source: str,\n):\n    \"\"\"Add to a list of suspect bad source datasets\"\"\"\n    self.filter.append(source)\n</code></pre>"},{"location":"api/#osm_merge.conflateBuildings.ConflateBuildings.overlapDB","title":"overlapDB","text":"<pre><code>overlapDB(dburi)\n</code></pre> <p>Conflate buildings where all the data is in the same postgres database using the Underpass raw data schema.</p> <p>Parameters:</p> Name Type Description Default <code>dburi</code> <code>str</code> <p>The URI for the existing OSM data</p> required <p>This is not fast for large areas!</p> Source code in <code>osm_merge/conflateBuildings.py</code> <pre><code>def overlapDB(\n    self,\n    dburi: str,\n):\n    \"\"\"Conflate buildings where all the data is in the same postgres database\n    using the Underpass raw data schema.\n\n    Args:\n        dburi (str): The URI for the existing OSM data\n\n    This is not fast for large areas!\n    \"\"\"\n    timer = Timer(text=\"conflateData() took {seconds:.0f}s\")\n    timer.start()\n    # Find duplicate buildings in the same database\n    # sql = f\"DROP VIEW IF EXISTS overlap_view;CREATE VIEW overlap_view AS SELECT ST_Area(ST_INTERSECTION(g1.geom::geography, g2.geom::geography)) AS area,g1.osm_id AS id1,g1.geom as geom1,g2.osm_id AS id2,g2.geom as geom2 FROM {self.view} AS g1, {self.view} AS g2 WHERE ST_OVERLAPS(g1.geom, g2.geom) AND (g1.tags-&gt;&gt;'building' IS NOT NULL AND g2.tags-&gt;&gt;'building' IS NOT NULL)\"\n    # sql = \"SELECT * FROM (SELECT ways_view.id, tags, ROW_NUMBER() OVER(PARTITION BY geom ORDER BY ways_view.geom asc) AS Row, geom FROM ONLY ways_view) dups WHERE dups.Row &gt; 1\"\n    # Make a new postgres VIEW of all overlapping or touching buildings\n    # log.info(f\"Looking for overlapping buildings in \\\"{self.uri['dbname']}\\\", this make take awhile...\")\n    # print(sql)\n    # Views must be dropped in the right order\n    sql = (\n        \"DROP TABLE IF EXISTS dups_view CASCADE; DROP TABLE IF EXISTS osm_view CASCADE;DROP TABLE IF EXISTS ways_view CASCADE;\"\n    )\n    result = self.db.queryDB(sql)\n\n    if self.boundary:\n        self.db.clipDB(self.boundary)\n\n    log.debug(\"Clipping OSM database\")\n    ewkt = shape(self.boundary)\n    uri = uriParser(dburi)\n    log.debug(f\"Extracting OSM subset from \\\"{uri['dbname']}\\\"\")\n    sql = f\"CREATE TABLE osm_view AS SELECT osm_id,tags,geom FROM dblink('dbname={uri['dbname']}', 'SELECT osm_id,tags,geom FROM ways_poly') AS t1(osm_id int, tags jsonb, geom geometry) WHERE ST_CONTAINS(ST_GeomFromEWKT('SRID=4326;{ewkt}'), geom) AND tags-&gt;&gt;'building' IS NOT NULL\"\n    # print(sql)\n    result = self.db.queryDB(sql)\n\n    sql = \"CREATE TABLE dups_view AS SELECT ST_Area(ST_INTERSECTION(g1.geom::geography, g2.geom::geography)) AS area,g1.osm_id AS id1,g1.geom as geom1,g1.tags AS tags1,g2.osm_id AS id2,g2.geom as geom2, g2.tags AS tags2 FROM ways_view AS g1, osm_view AS g2 WHERE ST_INTERSECTS(g1.geom, g2.geom) AND g2.tags-&gt;&gt;'building' IS NOT NULL\"\n    print(sql)\n    result = self.db.queryDB(sql)\n</code></pre>"},{"location":"api/#osm_merge.conflateBuildings.ConflateBuildings.cleanDuplicates","title":"cleanDuplicates","text":"<pre><code>cleanDuplicates()\n</code></pre> <p>Delete the entries from the duplicate building view.</p> <p>Returns:</p> Type Description <code>FeatureCollection</code> <p>The entries from the datbase table</p> Source code in <code>osm_merge/conflateBuildings.py</code> <pre><code>def cleanDuplicates(self):\n    \"\"\"Delete the entries from the duplicate building view.\n\n    Returns:\n        (FeatureCollection): The entries from the datbase table\n    \"\"\"\n    log.debug(\"Removing duplicate buildings from ways_view\")\n    sql = \"DELETE FROM ways_view WHERE osm_id IN (SELECT id1 FROM dups_view)\"\n\n    result = self.db.queryDB(sql)\n    return True\n</code></pre>"},{"location":"api/#osm_merge.conflateBuildings.ConflateBuildings.getNew","title":"getNew","text":"<pre><code>getNew()\n</code></pre> <p>Get only the new buildings</p> <p>Returns:</p> Type Description <code>FeatureCollection</code> <p>The entries from the datbase table</p> Source code in <code>osm_merge/conflateBuildings.py</code> <pre><code>def getNew(self):\n    \"\"\"Get only the new buildings\n\n    Returns:\n        (FeatureCollection): The entries from the datbase table\n    \"\"\"\n    sql = \"SELECT osm_id,geom,tags FROM ways_view\"\n    result = self.db.queryDB(sql)\n    features = list()\n    for item in result:\n        # log.debug(item)\n        entry = {\"osm_id\": item[0]}\n        entry.update(item[2])\n        geom = wkb.loads(item[1])\n        features.append(Feature(geometry=geom, properties=entry))\n\n    log.debug(f\"{len(features)} new features found\")\n    return FeatureCollection(features)\n</code></pre>"},{"location":"api/#osm_merge.conflateBuildings.ConflateBuildings.findHighway","title":"findHighway","text":"<pre><code>findHighway(feature)\n</code></pre> <p>Find the nearest highway to a feature</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>Feature</code> <p>The feature to check against</p> required Source code in <code>osm_merge/conflateBuildings.py</code> <pre><code>def findHighway(\n    self,\n    feature: Feature,\n):\n    \"\"\"Find the nearest highway to a feature\n\n    Args:\n        feature (Feature): The feature to check against\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#osm_merge.conflateBuildings.ConflateBuildings.getDuplicates","title":"getDuplicates","text":"<pre><code>getDuplicates()\n</code></pre> <p>Get the entries from the duplicate building view.</p> <p>Returns:</p> Type Description <code>FeatureCollection</code> <p>The entries from the datbase table</p> Source code in <code>osm_merge/conflateBuildings.py</code> <pre><code>def getDuplicates(self):\n    \"\"\"Get the entries from the duplicate building view.\n\n    Returns:\n        (FeatureCollection): The entries from the datbase table\n    \"\"\"\n    sql = \"SELECT area,id1,geom1,tags1,id2,geom2,tags2 FROM dups_view\"\n    result = self.db.queryDB(sql)\n    features = list()\n    for item in result:\n        # log.debug(item)\n        # First building identified\n        entry = {\"area\": float(item[0]), \"id\": int(item[1])}\n        geom = wkb.loads(item[2])\n        entry.update(item[3])\n        features.append(Feature(geometry=geom, properties=entry))\n\n        # Second building identified\n        entry = {\"area\": float(item[0]), \"id\": int(item[4])}\n        entry[\"id\"] = int(item[4])\n        geom = wkb.loads(item[5])\n        entry.update(item[6])\n        # FIXME: Merge the tags from the buildings into the OSM feature\n        # entry.update(item[3])\n        features.append(Feature(geometry=geom, properties=entry))\n\n    log.debug(f\"{len(features)} duplicate features found\")\n    return FeatureCollection(features)\n</code></pre>"},{"location":"api/#conflatepoipy","title":"conflatePOI.py","text":"<p>             Bases: <code>object</code></p> <p>Parameters:</p> Name Type Description Default <code>dburi</code> <code>str</code> <p>The DB URI</p> <code>None</code> <code>boundary</code> <code>Polygon</code> <p>The AOI of the project</p> <code>None</code> <code>threshold</code> <code>int</code> <p>The distance in meters for distance calculations</p> <code>7</code> <p>Returns:</p> Type Description <code>ConflatePOI</code> <p>An instance of this object</p> Source code in <code>osm_merge/conflatePOI.py</code> <pre><code>def __init__(self,\n             dburi: str = None,\n             boundary: Polygon = None,\n             threshold: int = 7,\n             ):\n    \"\"\"\n    This class conflates data that has been imported into a postgres\n    database using the Underpass raw data schema.\n\n    Args:\n        dburi (str): The DB URI\n        boundary (Polygon): The AOI of the project\n        threshold (int): The distance in meters for distance calculations\n\n    Returns:\n        (ConflatePOI): An instance of this object\n    \"\"\"\n    self.data = dict()\n    self.db = None\n    self.tolerance = threshold # Distance in meters for conflating with postgis\n    self.boundary = boundary\n    # Use a common select so it's consistent when parsing results\n    self.select = \"SELECT osm_id,tags,version,ST_AsText(geom),ST_Distance(geom::geography, ST_GeogFromText(\\'SRID=4326;%s\\'))\"\n    if dburi:\n        # for thread in range(0, cores + 1):\n        self.db = GeoSupport(dburi)\n        # self.db.append(db)\n        # We only need to clip the database into a new table once\n        if boundary:\n            self.db.clipDB(boundary, self.db.db)\n            self.db.clipDB(boundary, self.db.db, \"nodes_view\", \"nodes\")\n</code></pre> <p>options: show_source: false heading_level: 3</p>"},{"location":"api/#osm_merge.conflatePOI.ConflatePOI.overlaps","title":"overlaps","text":"<pre><code>overlaps(feature)\n</code></pre> <p>Conflate a POI against all the features in a GeoJson file</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>dict</code> <p>The feature to conflate</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The modified feature</p> Source code in <code>osm_merge/conflatePOI.py</code> <pre><code>def overlaps(self,\n            feature: dict,\n            ):\n    \"\"\"\n    Conflate a POI against all the features in a GeoJson file\n\n    Args:\n        feature (dict): The feature to conflate\n\n    Returns:\n        (dict):  The modified feature\n    \"\"\"\n    # Most smartphone GPS are 5-10m off most of the time, plus sometimes\n    # we're standing in front of an amenity and recording that location\n    # instead of in the building.\n    gps_accuracy = 10\n    # this is the treshold for fuzzy string matching\n    match_threshold = 80\n    # log.debug(f\"conflateFile({feature})\")\n    hits = False\n    data = dict()\n    geom = Point((float(feature[\"attrs\"][\"lon\"]), float(feature[\"attrs\"][\"lat\"])))\n    wkt = shape(geom)\n    for existing in self.data['features']:\n        id = int(existing['properties']['id'])\n        entry = shapely.from_geojson(str(existing))\n        if entry.geom_type != 'Point':\n            center = shapely.centroid(entry)\n        else:\n            center = entry\n            # dist = shapely.hausdorff_distance(center, wkt)\n            # if 'name' in existing['properties']:\n            #     print(f\"DIST1: {dist}, {existing['properties']['name']}\")\n        # x = shapely.distance(wkt, entry)\n        # haversine reverses the order of lat &amp; lon from what shapely uses. We\n        # use this as meters is easier to deal with than cartesian coordinates.\n        x1 = (center.coords[0][1], center.coords[0][0])\n        x2 = (wkt.coords[0][1], wkt.coords[0][0])\n        dist = haversine(x1, x2, unit=Unit.METERS)\n        if dist &lt; gps_accuracy:\n            # if 'name' in existing['properties']:\n            # log.debug(f\"DIST2: {dist}\")\n            # log.debug(f\"Got a Hit! {feature['tags']['name']}\")\n            for key,value in feature['tags'].items():\n                if key in self.analyze:\n                    if key in existing['properties']:\n                        result = fuzz.ratio(value, existing['properties'][key])\n                        if result &gt; match_threshold:\n                            # log.debug(f\"Matched: {result}: {feature['tags']['name']}\")\n                            existing['properties']['fixme'] = \"Probably a duplicate!\"\n                            log.debug(f\"Got a dup in file!!! {existing['properties']['name'] }\")\n                            hits = True\n                            break\n        if hits:\n            version = int(existing['properties']['version'])\n            # coords = feature['geometry']['coordinates']\n            # lat = coords[1]\n            # lon = coords[0]\n            attrs = {'id': id, 'version': version, 'lat': feature['attrs']['lat'], 'lon': feature['attrs']['lon']}\n            tags = existing['properties']\n            tags['fixme'] = \"Probably a duplicate!\"\n            # Data extracts for ODK Collect\n            del tags['title']\n            del tags['label']\n            if 'building' in tags:\n                return {'attrs': attrs, 'tags': tags, 'refs': list()}\n            return {'attrs': attrs, 'tags': tags}\n    return dict()\n</code></pre>"},{"location":"api/#osm_merge.conflatePOI.ConflatePOI.queryToFeature","title":"queryToFeature","text":"<pre><code>queryToFeature(results)\n</code></pre> <p>Convert the results of an SQL to a GeoJson Feature</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The results of the query</p> required <p>Returns:</p> Type Description <code>list</code> <p>a list of the features fromn the results</p> Source code in <code>osm_merge/conflatePOI.py</code> <pre><code>def queryToFeature(self,\n                   results: list,\n                   ):\n    \"\"\"\n    Convert the results of an SQL to a GeoJson Feature\n\n    Args:\n        results (list): The results of the query\n\n    Returns:\n        (list): a list of the features fromn the results\n    \"\"\"\n\n    features = list()\n    for entry in results:\n        osm_id = int(entry[0])\n        tags = entry[1]\n        version = int(entry[2])\n        coords = shapely.from_wkt(entry[3])\n        dist = entry[4]\n        # ways have an additional column\n        if len(entry) == 6:\n            refs = entry[5]\n        else:\n            refs = list()\n        if coords.geom_type == 'Polygon':\n            center = shapely.centroid(coords)\n            lat = center.y\n            lon = center.x\n            tags['geom_type'] = 'way'\n        elif coords.geom_type == \"Point\":\n            lat = coords.y\n            lon = coords.x\n            tags['geom_type'] = 'node'\n        else:\n            log.error(f\"Unsupported geometry type: {coords.geom_type}\")\n        # match = entry[5] # FIXME: for debugging\n        # the timestamp attribute gets added when it's uploaded to OSM.\n        attrs = {'id': osm_id,\n                'version': version,\n                'lat': lat,\n                'lon': lon,\n                }\n        tags['dist'] = dist\n        # tags['match'] = match # FIXME: for debugging\n        # tags['fixme'] = \"Probably a duplicate node!\"\n        features.append({'attrs': attrs, 'tags': tags, 'refs': refs})\n\n    return features\n</code></pre>"},{"location":"api/#osm_merge.conflatePOI.ConflatePOI.checkTags","title":"checkTags","text":"<pre><code>checkTags(feature, osm)\n</code></pre> <p>Check tags between 2 features.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>Feature</code> <p>The feature from the external dataset</p> required <code>osm</code> <code>dict</code> <p>The result of the SQL query</p> required <p>Returns:</p> Type Description <code>int</code> <p>The nunber of tag matches</p> <code>dict</code> <p>The updated tags</p> Source code in <code>osm_merge/conflatePOI.py</code> <pre><code>def checkTags(self,\n              feature: Feature,\n              osm: dict,\n              ):\n    \"\"\"\n    Check tags between 2 features.\n\n    Args:\n        feature (Feature): The feature from the external dataset\n        osm (dict): The result of the SQL query\n\n    Returns:\n        (int): The nunber of tag matches\n        (dict): The updated tags\n    \"\"\"\n    tags = osm['tags']\n    hits = 0\n    match_threshold = 80\n    if osm['tags']['dist'] &gt; float(self.tolerance):\n        return 0, osm['tags']\n    for key, value in feature['tags'].items():\n        if key in tags:\n            ratio = fuzz.ratio(value, tags[key])\n            if ratio &gt; match_threshold:\n                hits += 1\n            else:\n                if key != 'note':\n                    tags[f'old_{key}'] = value\n        tags[key] = value\n\n    return hits, tags\n</code></pre>"},{"location":"api/#osm_merge.conflatePOI.ConflatePOI.conflateData","title":"conflateData","text":"<pre><code>conflateData(data, threshold=7)\n</code></pre> <p>Conflate all the data. This the primary interfacte for conflation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list</code> <p>A list of all the entries in the OSM XML input file</p> required <code>threshold</code> <code>int</code> <p>The threshold for distance calculations</p> <code>7</code> <p>Returns:</p> Type Description <code>dict</code> <p>The modified features</p> Source code in <code>osm_merge/conflatePOI.py</code> <pre><code>def conflateData(self,\n                 data: list,\n                 threshold: int = 7,\n                 ):\n    \"\"\"\n    Conflate all the data. This the primary interfacte for conflation.\n\n    Args:\n        data (list): A list of all the entries in the OSM XML input file\n        threshold (int): The threshold for distance calculations\n\n    Returns:\n        (dict):  The modified features\n    \"\"\"\n    timer = Timer(text=\"conflateData() took {seconds:.0f}s\")\n    timer.start()\n    # Use fuzzy string matching to handle minor issues in the name column,\n    # which is often used to match an amenity.\n    if len(self.data) == 0:\n        self.db.queryDB(\"CREATE EXTENSION IF NOT EXISTS fuzzystrmatch\")\n    log.debug(f\"conflateData() called! {len(data)} features\")\n\n    # A chunk is a group of threads\n    entries = len(data)\n    chunk = round(len(data) / cores)\n\n    if True: # FIXME: entries &lt;= chunk:\n        result = conflateThread(data, self)\n        timer.stop()\n        return result\n\n    # Chop the data into a subset for each thread\n    newdata = list()\n    future = None\n    result = None\n    index = 0\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cores) as executor:\n        i = 0\n        subset = dict()\n        futures = list()\n        for key, value in data.items():\n            subset[key] = value\n            if i == chunk:\n                i = 0\n                result = executor.submit(conflateThread, subset, self)\n                index += 1\n                # result.add_done_callback(callback)\n                futures.append(result)\n                subset = dict()\n            i += 1\n        for future in concurrent.futures.as_completed(futures):\n            log.debug(f\"Waiting for thread to complete..\")\n            # print(f\"YYEESS!! {future.result(timeout=10)}\")\n            newdata.append(future.result(timeout=5))\n    timer.stop()\n    return newdata\n</code></pre>"},{"location":"api/#osm_merge.conflatePOI.ConflatePOI.queryWays","title":"queryWays","text":"<pre><code>queryWays(feature, db=None)\n</code></pre> <p>Conflate a POI against all the ways in a postgres view</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>Feature</code> <p>The feature to conflate</p> required <code>db</code> <code>GeoSupport</code> <p>The datbase connection to use</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>The data with tags added from the conflation</p> Source code in <code>osm_merge/conflatePOI.py</code> <pre><code>    def queryWays(self,\n                    feature: Feature,\n                    db: GeoSupport = None,\n                    ):\n        \"\"\"\n        Conflate a POI against all the ways in a postgres view\n\n        Args:\n            feature (Feature): The feature to conflate\n            db (GeoSupport): The datbase connection to use\n\n        Returns:\n            (list): The data with tags added from the conflation\n        \"\"\"\n        # log.debug(f\"conflateWay({feature})\")\n        hits = 0\n        result = list()\n        geom = Point((float(feature[\"attrs\"][\"lon\"]), float(feature[\"attrs\"][\"lat\"])))\n        wkt = shape(geom)\n\n        # cleanval = escape(value)\n        # Get all ways close to this feature.\n#        query = f\"SELECT osm_id,tags,version,ST_AsText(ST_Centroid(geom)),ST_Distance(geom::geography, ST_GeogFromText(\\'SRID=4326;{wkt.wkt}\\')) FROM ways_view WHERE ST_Distance(geom::geography, ST_GeogFromText(\\'SRID=4326;{wkt.wkt}\\')) &lt; {self.tolerance} ORDER BY ST_Distance(geom::geography, ST_GeogFromText(\\'SRID=4326;{wkt.wkt}\\'))\"\n        query = f\"{self.select}\" % wkt.wkt\n        query += f\", refs FROM ways_view WHERE ST_Distance(geom::geography, ST_GeogFromText(\\'SRID=4326;{wkt.wkt}\\')) &lt; {self.tolerance} ORDER BY ST_Distance(geom::geography, ST_GeogFromText(\\'SRID=4326;{wkt.wkt}\\'))\"\n        #log.debug(query)\n        result = list()\n        if db:\n            result = db.queryDB(query)\n        else:\n            result = self.db.queryDB(query)\n        if len(result) &gt; 0:\n            hits += 1\n        else:\n            log.warning(f\"No results at all for {query}\")\n\n        return result\n</code></pre>"},{"location":"api/#osm_merge.conflatePOI.ConflatePOI.queryNodes","title":"queryNodes","text":"<pre><code>queryNodes(feature, db=None)\n</code></pre> <p>Find all the nodes in the view within a certain distance that are buildings or amenities.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>Feature</code> <p>The feature to use as the location</p> required <code>db</code> <code>GeoSupport</code> <p>The database connection to use</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>The results of the conflation</p> Source code in <code>osm_merge/conflatePOI.py</code> <pre><code>def queryNodes(self,\n                 feature: Feature,\n                 db: GeoSupport = None,\n                 ):\n    \"\"\"\n    Find all the nodes in the view within a certain distance that\n    are buildings or amenities.\n\n    Args:\n        feature (Feature): The feature to use as the location\n        db (GeoSupport): The database connection to use\n\n    Returns:\n        (list): The results of the conflation\n    \"\"\"\n    # log.debug(f\"queryNodes({feature})\")\n    hits = 0\n    geom = Point((float(feature[\"attrs\"][\"lon\"]), float(feature[\"attrs\"][\"lat\"])))\n    wkt = shape(geom)\n    result = list()\n    ratio = 1\n\n    # for key,value in feature['tags'].items():\n    # print(f\"NODE: {key} = {value}\")\n    # if key not in self.analyze:\n    #     continue\n\n    # Use a Geography data type to get the answer in meters, which\n    # is easier to deal with than degress of the earth.\n    # cleanval = escape(value)\n    # query = f\"SELECT osm_id,tags,version,ST_AsEWKT(geom),ST_Distance(geom::geography, ST_GeogFromText(\\'SRID=4326;{wkt.wkt}\\')),levenshtein(tags-&gt;&gt;'{key}', '{cleanval}') FROM nodes_view WHERE ST_Distance(geom::geography, ST_GeogFromText(\\'SRID=4326;{wkt.wkt}\\')) &lt; {self.tolerance} AND levenshtein(tags-&gt;&gt;'{key}', '{cleanval}') &lt;= {ratio}\"\n    # AND (tags-&gt;&gt;'amenity' IS NOT NULL OR tags-&gt;&gt;'shop' IS NOT NULL)\"\n    query = f\"{self.select}\" % wkt.wkt\n    query += f\" FROM nodes_view WHERE ST_Distance(geom::geography, ST_GeogFromText(\\'SRID=4326;{wkt.wkt}\\')) &lt; {self.tolerance} AND (tags-&gt;&gt;'amenity' IS NOT NULL OR tags-&gt;&gt;'building' IS NOT NULL)\"\n    #log.debug(query)\n    # FIXME: this currently only works with a local database,\n    # not underpass yet\n    if db:\n        result = db.queryDB(query)\n    else:\n        result = self.db.queryDB(query)\n    # log.debug(f\"Got {len(result)} results\")\n    if len(result) &gt; 0:\n        hits += 1\n        # break\n    # else:\n    #     log.warning(f\"No results at all for {query}\")\n\n    return result\n</code></pre>"},{"location":"api/#geosupportpy","title":"geosupport.py","text":"<p>             Bases: <code>object</code></p> <p>Parameters:</p> Name Type Description Default <code>dburi</code> <code>str</code> <p>The database URI</p> <code>None</code> <code>config</code> <code>str</code> <p>The config file from the osm-rawdata project</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoSupport</code> <p>An instance of this object</p> Source code in <code>osm_merge/geosupport.py</code> <pre><code>def __init__(self,\n             dburi: str = None,\n             config: str = None,\n             ):\n    \"\"\"\n    This class conflates data that has been imported into a postgres\n    database using the Underpass raw data schema.\n\n    Args:\n        dburi (str, optional): The database URI\n        config (str, optional): The config file from the osm-rawdata project\n\n    Returns:\n        (GeoSupport): An instance of this object\n    \"\"\"\n    self.db = None\n    self.dburi = dburi\n    self.config = config\n</code></pre> <p>options: show_source: false heading_level: 3</p>"},{"location":"api/#osm_merge.geosupport.GeoSupport.importDataset","title":"importDataset  <code>async</code>","text":"<pre><code>importDataset(filespec)\n</code></pre> <p>Import a GeoJson file into a postgres database for conflation.</p> <p>Parameters:</p> Name Type Description Default <code>filespec</code> <code>str</code> <p>The GeoJson file to import</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the import was successful</p> Source code in <code>osm_merge/geosupport.py</code> <pre><code>async def importDataset(self,\n                 filespec: str,\n                 ) -&gt; bool:\n    \"\"\"\n    Import a GeoJson file into a postgres database for conflation.\n\n    Args:\n        filespec (str): The GeoJson file to import\n\n    Returns:\n        (bool): If the import was successful\n    \"\"\"\n    file = open(filespec, \"r\")\n    data = geojson.load(file)\n\n    # Create the tables\n    sql = \"CREATE EXTENSION postgis;\"\n    result = await self.db.execute(sql)\n    sql = f\"DROP TABLE IF EXISTS public.nodes CASCADE; CREATE TABLE public.nodes (osm_id bigint, geom geometry, tags jsonb);\"\n    result = await self.db.execute(sql)\n    sql = f\"DROP TABLE IF EXISTS public.ways_line CASCADE; CREATE TABLE public.ways_line (osm_id bigint, geom geometry, tags jsonb);\"\n    result = await self.db.execute(sql)\n    sql = f\"DROP TABLE IF EXISTS public.poly CASCADE; CREATE TABLE public.ways_poly (osm_id bigint, geom geometry, tags jsonb);\"\n    result = await self.db.execute(sql)\n\n    # if self.db.is_closed():\n    #     return False\n\n    table = self.dburi.split('/')[1]\n    for entry in data[\"features\"]:\n        keys = \"geom, \"\n        geometry = shape(entry[\"geometry\"])\n        ewkt = geometry.wkt\n        if geometry.geom_type == \"LineString\":\n            table = \"ways_line\"\n        if geometry.geom_type == \"Polygon\":\n            table = \"ways_poly\"\n        if geometry.geom_type == \"Point\":\n            table = \"nodes\"\n        tags = f\"\\'{{\"\n        for key, value in entry[\"properties\"].items():\n            tags += f\"\\\"{key}\\\": \\\"{value}\\\", \"\n        tags = tags[:-2]\n        tags += \"}\\'::jsonb)\"\n        sql = f\"INSERT INTO {table} (geom, tags) VALUES(ST_GeomFromEWKT(\\'SRID=4326;{ewkt}\\'), {tags}\"\n        result = await self.db.pg.execute(sql)\n\n    return False\n</code></pre>"},{"location":"api/#osm_merge.geosupport.GeoSupport.initialize","title":"initialize  <code>async</code>","text":"<pre><code>initialize(dburi=None, config=None)\n</code></pre> <p>When async, we can't initialize the async database connection, so it has to be done as an extrat step.</p> <p>Parameters:</p> Name Type Description Default <code>dburi</code> <code>str</code> <p>The database URI</p> <code>None</code> <code>config</code> <code>str</code> <p>The config file from the osm-rawdata project</p> <code>None</code> Source code in <code>osm_merge/geosupport.py</code> <pre><code>async def initialize(self,\n                    dburi: str = None,\n                    config: str = None,\n                    ):\n    \"\"\"\n    When async, we can't initialize the async database connection,\n    so it has to be done as an extrat step.\n\n    Args:\n        dburi (str, optional): The database URI\n        config (str, optional): The config file from the osm-rawdata project\n    \"\"\"\n    if dburi:\n        self.db = PostgresClient()\n        await self.db.connect(dburi)\n    elif self.dburi:\n        self.db = PostgresClient()\n        await self.db.connect(self.dburi)\n\n    if config:\n        await self.db.loadConfig(config)\n    elif self.config:\n        await self.db.loadConfig(config)\n</code></pre>"},{"location":"api/#osm_merge.geosupport.GeoSupport.clipDB","title":"clipDB  <code>async</code>","text":"<pre><code>clipDB(boundary, db=None, view='ways_view')\n</code></pre> <p>Clip a database table by a boundary</p> <p>Parameters:</p> Name Type Description Default <code>boundary</code> <code>Polygon</code> <p>The AOI of the project</p> required <code>db</code> <code>PostgresClient</code> <p>A reference to the existing database connection</p> <code>None</code> <code>view</code> <code>str</code> <p>The name of the new view</p> <code>'ways_view'</code> <p>Returns:</p> Type Description <code>bool</code> <p>If the region was clipped sucessfully</p> Source code in <code>osm_merge/geosupport.py</code> <pre><code>async def clipDB(self,\n         boundary: Polygon,\n         db: PostgresClient = None,\n         view: str = \"ways_view\",\n         ):\n    \"\"\"\n    Clip a database table by a boundary\n\n    Args:\n        boundary (Polygon): The AOI of the project\n        db (PostgresClient): A reference to the existing database connection\n        view (str): The name of the new view\n\n    Returns:\n        (bool): If the region was clipped sucessfully\n    \"\"\"\n    remove = list()\n    if not boundary:\n        return False\n\n    ewkt = shape(boundary)\n\n    # Create a new postgres view\n    # FIXME: this should be a temp view in the future, this is to make\n    # debugging easier.\n    sql = f\"DROP VIEW IF EXISTS {view} CASCADE ;CREATE VIEW {view} AS SELECT * FROM ways_poly WHERE ST_CONTAINS(ST_GeomFromEWKT('SRID=4326;{ewkt}'), geom)\"\n    # log.debug(sql)\n    if db:\n        result = await db.queryDB(sql)\n    elif self.db:\n        result = await self.db.queryDBl(sql)\n    else:\n        return False\n\n    return True\n</code></pre>"},{"location":"api/#osm_merge.geosupport.GeoSupport.queryDB","title":"queryDB  <code>async</code>","text":"<pre><code>queryDB(sql=None, db=None)\n</code></pre> <p>Query a database table</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>PostgreClient</code> <p>A reference to the existing database connection</p> <code>None</code> <code>sql</code> <code>str</code> <p>The SQL query to execute</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>The results of the query</p> Source code in <code>osm_merge/geosupport.py</code> <pre><code>async def queryDB(self,\n            sql: str = None,\n            db: PostgresClient = None,\n            ) -&gt; list:\n    \"\"\"\n    Query a database table\n\n    Args:\n        db (PostgreClient, optional): A reference to the existing database connection\n        sql (str): The SQL query to execute\n\n    Returns:\n        (list): The results of the query\n    \"\"\"\n    result = list()\n    if not sql:\n        log.error(f\"You need to pass a valid SQL string!\")\n        return result\n\n    if db:\n        result = db.queryLocal(sql)\n    elif self.db:\n        result = self.db.queryLocal(sql)\n\n    return result\n</code></pre>"},{"location":"api/#osm_merge.geosupport.GeoSupport.clipFile","title":"clipFile  <code>async</code>","text":"<pre><code>clipFile(boundary, data)\n</code></pre> <p>Clip a database table by a boundary</p> <p>Parameters:</p> Name Type Description Default <code>boundary</code> <code>Polygon</code> <p>The filespec of the project AOI</p> required <code>data</code> <code>FeatureCollection</code> <p>The data to clip</p> required <p>Returns:</p> Type Description <code>FeatureCollection</code> <p>The data within the boundary</p> Source code in <code>osm_merge/geosupport.py</code> <pre><code>async def clipFile(self,\n            boundary: Polygon,\n            data: FeatureCollection,\n            ):\n    \"\"\"\n    Clip a database table by a boundary\n\n    Args:\n        boundary (Polygon): The filespec of the project AOI\n        data (FeatureCollection): The data to clip\n\n    Returns:\n        (FeatureCollection): The data within the boundary\n    \"\"\"\n    new = list()\n    if len(self.data) &gt; 0:\n        for feature in self.data[\"features\"]:\n            shapely.from_geojson(feature)\n            if not shapely.contains(ewkt, entry):\n                log.debug(f\"CONTAINS {entry}\")\n                new.append(feature)\n                #  del self.data[self.data['features']]\n\n    return new\n</code></pre>"},{"location":"api/#osm_merge.geosupport.GeoSupport.copyTable","title":"copyTable  <code>async</code>","text":"<pre><code>copyTable(table, remote)\n</code></pre> <p>Use DBLINK to copy a table from the external database to a local table so conflating is much faster.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>The table to copy</p> required Source code in <code>osm_merge/geosupport.py</code> <pre><code>async def copyTable(self,\n                    table: str,\n                    remote: PostgresClient,\n                    ):\n    \"\"\"\n    Use DBLINK to copy a table from the external\n    database to a local table so conflating is much faster.\n\n    Args:\n        table (str): The table to copy\n    \"\"\"\n    timer = Timer(initial_text=f\"Copying {table}...\",\n                  text=\"copying {table} took {seconds:.0f}s\",\n                  logger=log.debug,\n                )\n    # Get the columns from the remote database table\n    self.columns = await remote.getColumns(table)\n\n    print(f\"SELF: {self.pg.dburi}\")\n    print(f\"REMOTE: {remote.dburi}\")\n\n    # Do we already have a local copy ?\n    sql = f\"SELECT FROM pg_tables WHERE schemaname = 'public' AND tablename  = '{table}'\"\n    result = await self.pg.execute(sql)\n    print(result)\n\n    # cleanup old temporary tables in the current database\n    # drop = [\"DROP TABLE IF EXISTS users_bak\",\n    #         \"DROP TABLE IF EXISTS user_interests\",\n    #         \"DROP TABLE IF EXISTS foo\"]\n    # result = await pg.pg.executemany(drop)\n    sql = f\"DROP TABLE IF EXISTS new_{table} CASCADE\"\n    result = await self.pg.execute(sql)\n    sql = f\"DROP TABLE IF EXISTS {table}_bak CASCADE\"\n    result = await self.pg.execute(sql)\n    timer.start()\n    dbuser = self.pg.dburi[\"dbuser\"]\n    dbpass = self.pg.dburi[\"dbpass\"]\n    sql = f\"CREATE SERVER IF NOT EXISTS pg_rep_db FOREIGN DATA WRAPPER dblink_fdw  OPTIONS (dbname 'tm4');\"\n    data = await self.pg.execute(sql)\n\n    sql = f\"CREATE USER MAPPING IF NOT EXISTS FOR {dbuser} SERVER pg_rep_db OPTIONS ( user '{dbuser}', password '{dbpass}');\"\n    result = await self.pg.execute(sql)\n\n    # Copy table from remote database so JOIN is faster when it's in the\n    # same database\n    #columns = await sel.getColumns(table)\n    log.warning(f\"Copying a remote table is slow, but faster than remote access......\")\n    sql = f\"SELECT * INTO {table} FROM dblink('pg_rep_db','SELECT * FROM {table}') AS {table}({self.columns})\"\n    print(sql)\n    result = await self.pg.execute(sql)\n\n    return True\n</code></pre>"},{"location":"conflation/","title":"Conflating External Datasets","text":"<p>This project is the merging of several programs for conflating external datasets with OpenStreetMap data developed at HOT. These were originally developed for large scale building imports using MS Footprints in East Africa, and to also work with conflating data collected with OpenDataKit for the Field Mapping Tasking Manager project.</p>"},{"location":"conflation/#the-data-files","title":"The Data Files","text":"<p>While any name can be used for the OSM database, I usually default to naming the OpenStreetMap database the country name as used in the data file. Other datasets have their own schema, and can be imported with ogr2ogr, or using python to write a custom importer. In that case I name the database after the dataset source. Past versions of this program could conflate between multiple datasets, so it's good to keep things clear.</p>"},{"location":"conflation/#overture-data","title":"Overture Data","text":"<p>The Overture Foundation (https://www.overturemaps.org) has been recently formed to build a competitor to Google Maps. The plan is to use OpenStreetMap (OSM) data as a baselayer, and layer other datasets on top. The currently available data (July 2023) has 13 different datasets in addition to the OSM data. It is available here. It also includes a snapshot  of OSM data from the same time frame. Other than the OSM data and MS Footprints, all the current additional data is primarily US specific, and often contains multiple copies of the same dataset, but from different organization.</p> <p>The osm-rawdata python module has a utility that'll import the Parquet data files into the postgress database schema used by multiple projects at HOT. That schema is designed for data analysis, unlike the standard OSM database schema. There is more detail in these notes I've written about importing Overture Data into postgres.</p>"},{"location":"conflation/#duplicate-buildings","title":"Duplicate Buildings","text":"<p>This is the primary conflation task. Because of offsets in the satellite imagery used for the original buildings, there is rarely an exact duplicate, only similar. The only times when you see an exact duplicate, it's because the same source data is in multiple other datasets. The orientation may be different even if the same rough size, or it'll be roughly in the same position, but differing sizes. Several checks are made to determine duplicates. First is to check for any intersection of the two polygons. If the two polygons intersection it's an overlapping building or possibly duplicate. Any building in the footprint data that is found to be a duplicate is removed from the output data file.</p>"},{"location":"conflation/#overlapping-buildings","title":"Overlapping Buildings","text":"<p>It is entirely possible that a new building in the footprints data may overlap with an existing building in OSM. It wouldn't be overlapping in the footprints data. Since this requires human intervention to fix, these buildings are left in the output data, but flagged with a debugging tag of overlapping=yes. There is also many occurances where the building being imported has a better building geometry than OSM, so the best one should be selected.</p> <p>Using the HOT Underpass project, it is possible to scan the building geometries and either delete the bad geometry one, or flag it in the result data files for a human to validate the results.</p>"},{"location":"conflation/#known-problems","title":"Known Problems","text":"<p>There are two main issues with ML/AI derived building footprints, Buildings that are very close together, like the business section in many areas of the world, do not get marked as separate buildings. Instead the entire block of buildings is a single polygon. This will eventually get fixed by drone mapping, where there can be more of a street view of the buildings that you can't get using existing satellite imagery.</p> <p>The other problem is that as processing satellite imagery is that buildings are recognized by shading differences, so often features are flagged as buildings that don't actually exist. For example, big rocks in the desert, or haystacks in a field both get marked as a building. Any building in the footprints data that has no other buildings nearby, nor a highway or path of some kind, is flagged with a debugging tag of false=yes. Usually this is easy to determine looking at satellite imagery, since these are often remote buildings. The tags can be searched for when editing the data to visually determine whether it's a real building or not.</p>"},{"location":"conflation/#conflating-other-than-buildings","title":"Conflating Other Than Buildings","text":""},{"location":"conflation/#opendatakit","title":"OpenDataKit","text":"<p>Data collected in the field using ODK Collect is a specific case. If using using data extracts from OpenStreetMap, the data extract has the OSM ID, so it's much simpler to conflate the new tags with either the existing building polygon or POI. For this workflow, any tag in the feature from ODK will overwrite any existing values in the existing feature. This allows for updating the tags &amp; values when ground-truthing. When the OSM XML file is loaded into JOSM, it has the modified attribute set, and the version has been incremented. In JOSM under the File menu, select the Update Modified menu item. This will sync the modified feature with current OSM. At that point all that needs to be done is validate the modified features, and upload to OSM.</p> <p>When ODK Collect is used but has no data extract, conflation is more complicated. For this use case, a more brute force algorythm is used. Initially any building polygon or POI within 7 meters is found by querying the database. Most smartphone GPS chipsets, even on high-end phones, are between 4-9m off from your actual location. That value was derived by looking at lots of data, and can be changed when invoking the conflation software in this project. Once nearby buildings are identified, then the tags are compared to see if there is a match.</p> <p>For example, if collecting data on a restaurant, it may have a new name, but if the nearby building is the only one with an amenity=restaurant** (or cafe, pub, etc...) it's considered a probable match. If there are multiple restaurants this doesn't work very well unless the name hasn't changed. If there are multiple possible features, a *fixme= tag is added to the POI, and it has to be later validated manually. Every tag in the ODK data has to be compares with the nearby buildings. Often it's the name tag that is used for many amenities.</p> <p>If a satellite imagery basemap is used in Collect, conflation is somewhat simpler. If the mapper has selected the center of the building using the basemap, conflation starts by checking for the building polygon in OSM that contains this location. If no building is found, the POI is added to the output file with a fixme=new building tag so the buildings can traced by the validator. Any tags from the POI are added to the new building polygon.</p>"},{"location":"conflation/#points-of-interest-poi","title":"Points Of Interest (POI)","text":"<p>It is common when collecting datasets from non-OSM sources each feature may only be single node. This may be a list of schools, businesses, etc... with additional information with each POI that can be added to the OSM building polygon (if it exists). Obviously any imported data must have a license acceptable for importing into OSM.</p> <p>Similar to how conflating ODK data when not using a data extract, the tags &amp; values are compared with any nearby building. Since often these imports are features already in OSM with limited metadata, this adds more details.</p>"},{"location":"conflation/#highways","title":"Highways","text":"<p>Highways are more complex because it uses relations. A relation is a groups of highway segments into a single entity. Some times the tags are on the relation, other times each highway segment. The segments change when the highway condition changes, but the name and reference number doesn't change. External datasets don't use relations, they are OSM specific.</p>"},{"location":"conflation/#mvum-highways","title":"MVUM Highways","text":"<p>The USDA publishes a dataset of Motor Vehicle Use Maps (MVUM) highways in the National Forest. Some of this data has already been imported into OSM, although the metadata may be lacking, but the LineString is there. MVUM roads are primarily compacted dirt roads. While some can be driven in a passenger vehicle, most are varying degrees of bad to horrible to impassable. These highways are often used for recreational traffic by off-road vehicles, or for emergency access for a wildland fire or backcountry rescue.</p> <p>Another key detail of MVUM highways is each one may have 4 names! There is of course the primary name, for example \"Cedar Lake Road\". But it may also have a locals name, common in remote areas. And then there is the reference number. A MVUM highway may have two reference numbers, the country designated one, and the USDA one. Luckily OSM supports this. Many of these tags effect both how the highway is displayed, as well as routing for navigation. </p> <pre><code>\"name\": \"Platte Lake Road\",\n\"alt_name\": \"Bar-K Ranch Road\",\n\"surface\": \"dirt\",\n\"smoothness\": \"bad\",\n\"highway\": \"track\",\n\"ref\": \"CO 112\",\n\"ref:usfs\": \"FR 521.1A\"\n\"tracktype\": \"grade3\"\n</code></pre> <p>A bad highway is something I'd be comfortable driving in a 4x4 high-clearance vehicle. Smoothness values can be a bit misleading, as often what is in OSM may be years out of date. And most MVUM roads get zero maintainance, so get eroded, pot-holed, and or exposed rocks. And people's perception of road conditions is subjective based on one's experience driving these highways.</p> <p>All of this metadata makes conflation interesting. Since existing OSM features were added by more than one person, the tagging may not be consistent. For example, the existing data may have Forest Service Road 123, which should really be ref:usfs=FR 123. And the real highway name Piney Pass Road is in the MVUM dataset. The goal of highway conflation is to merge the new metadata into the existing OSM feature where possible. This then needs to be validated by a human being. There is still much tedious work to process post conflation data before it can be uploaded to OSM.</p> <p>But sometimes conflation works well, especially when the LineString in OSM was imported from older versions of the MVUM data. But often highways in OSM were traced off satellite imagery, and may have wildly different geometry.</p> <p>If you ignore conflating the tags other than name or ref, the process is somewhat less messy. And tags like surface and smoothness really should be ground-truthed anyway. So I do ignore those for now and stick to validating the name and the two reference numbers which are usually lacking in OSM. That and addding consistency to the data to make it easier to make data extracts.</p> <p>To conflate OSM highways with external data, initially each entry in the external dataset does a distance comparison with the existing OSM data. There is an optional threshold to set the distance limit. Since currently this is focused on conflating files without a database, this is computationally intensive, so slow. For data that was imported in the past from MVUM datasets, a distance of zero means it's probably the same segment. The external dataset needs to have the tagging converted to the syntax OSM uses. Tagging can be adjusted using a conversion program, but as conversion is usually a one-off task, it can also be done using JOSM or QGIS. Usually it's deleting most of the tags in the external dataset that aren't appropriate for OSM. Primarily the only tags that are needed are the name and any reference numbers. Since the MVUM data also classified the types of road surface, this can also be converted. Although as mentioned, may be drastically out of data, and OSM is more recent and ground-truthed.</p> <p>Then there is a comparison of the road names. It's assumed the one from the MVUM dataset is the correct one. And since typos and weird abbreviations may exist in the datasets, fuzzy string matching is performed. This way names like FS 123.1 can match FR 123.1A. In this case the current name value in OSM becomes alt_name, and the MVUM name becomes the official name. This way when validating you can make decisions where there is confusion on what is correct. For an exact name match no other tags are checked to save a little time.</p> <p>Any other processing is going to be MVUM highway specific, so there will be an additional step to work through the reference numbers not supported by this program.</p>"},{"location":"conflation/#output-files","title":"Output Files","text":"<p>If the data files are huge, it's necessary to conflate with a subset of all the data. For projects using the Tasking Manager or the Field Mapping Tasking Manager you can download the project boundary file and use that. For other projects you can extract administrative bondaries from OpenStreetMap, or use external sources. Usually county administrative boundaries are a good size. These can be extracted from OSM itself, or an external data file of boundaries.</p> <p>After conflation, an output file is created with the new buildings that are not duplicates of existing OSM data. This is much smaller than the original data, but still too large for anyone having bandwidth issues. This output file is in GeoJson format, so can be edited with JOSM or QGIS</p> <p>Since this software is under development, rather than automatically deleting features, it adds tags to the features. Then when editing the data, it's possible to see the flagged data and validate the conflation. It also makes it possible to delete manually the results of the conflation from the output file once satisfied about the validation of the results.</p>"},{"location":"conflation/#validating-the-conflation","title":"Validating The Conflation","text":"<p>The conflated data file can't be uploaded to OSM until it is validated. While QGIS can be used for this purpose, JOSM is preferred because it does validation checks, and uploads directly to OpenStreetMap. I start by loading the conflation data file, and then enabling the OpenStreetMap imagery for the basemap. Existing buildings in OSM are grey polygons, so it's possible to see existing buildings with the conflated new buildings as a layer on top.</p> <p>Once the buildings are loaded, you can then download the OSM data for that view. Then use the SelectDuplicateBuilding script to find any buildings that have been added since the initial data file for conflation was used. Once selected, those can be deleted in a single operation.</p> <p>The next step is validating what is left that is considered to be a new building. This is done using satellite imagery. Most commercial satellite imagery available for public use comes from Maxar. But the different providers (Bing, ESRI, Google, etc...) have different update cycles, so I often double check with ESRI imagery.</p> <p>If there is drone imagery available from Open Aerial Map, that's also a good surce of imagery, but often doesn't cover a large area.</p>"},{"location":"odkconflation/","title":"Conflating OpenDataKit with OpenStreetMap","text":"<p>Typically conflation is done when doing data imports, but not always. Data collected in the field can be considered an import. Conflating buildings or POIs from external data is relatively easy as it's already been cleaned up and validated. When you are doing field mapping, then you have to cleanup and validate the data during conflation. This is a time consuming process even with good conflation software.</p> <p>I've worked with multiple conflation software over the years. Hootenanny, OpenJump (later forked into RoadMatcher), etc...  which currently are now dead projects. Conflation is a hard technical challenge and often the results are poor and unstatisfing result. For smalller datasets often it's easier to do do manual conflation using JOSM or Qgis. This project tries to simply the problem by focusing on OpenStreetMap data.</p>"},{"location":"odkconflation/#smartphone-data-collection","title":"Smartphone Data Collection","text":"<p>While commercial organizations may use expensive GPS devices, most of us that do data collection as a volunteer or for an NGO use their smartphone. Their is a variety of smartphone apps for data collection that fall ihnto two categories. The first category are the apps like Vespucci, StreetComplete, and Organic Maps. These directly upload to OpenStreetMap. These are great for the casual mapper who only adds data occasionally and is limited to a POI. For example, a casual mapper may want to add the restaurant they are currrently eating in when they notices it's not in OpenStreetMap. In addition, they probably have a cell phone connection, so the data gets added right away.</p> <p>The other category are apps like ODK Collect, QField ArcGIS Field Maps which are oriented to larger scale mapping projects, often offline without any cellular connection. These collect a lot of data that then needs to get processed later. And conflation is part of this process.</p> <p>All of these smartphone based data collection apps suffer from poor GPS location accuracy. Modern smartphones (2024) are often 5-9 meters off the actual location, sometimes worse. In addition when field data collecting, you can't always record the actual location you want, you can only record where you are standing.</p> <p>You can improve the location data somewhat if you have a good quality basemap, for example you see a building within a courthouse wall when you are standing in the street. If you have a basemap, typically satellite imagery, you can touch the location on the basemap, and use that instead of where you are standing. Then later when conflating, you have a much higher chance the process will be less painful.</p>"},{"location":"odkconflation/#opendatakit","title":"OpenDataKit","text":"<p>OpenDataKit is a format for data import forms used to collect custom data. The source file is a spreadsheet, called an XLSForm. This gets used by the mobile app for the quesion and answer process defined by the XLSForm. There are multiple apps and projects using XLSForms, so it's well supported and maintained.</p> <p>The XLS source file syntax is a bit wierd at first, being a spreadsheet, so the osm-fieldwork project contains tested XLSForm templates for a variety of mapping project goals. These can be used to create efficient XForms that are easy to convert to OSM. The primary task when manually converting ODK collected data into OSM format is converting the tags. If the XLSForm is created with a focus towards OSM the XLSForm can make this a much simpler process. This is detailed more in this document. Simply stated, what is in the name colum in the XLSForm becomes the name of the tag in OSM, and the response from the choices sheet becomes the value.</p>"},{"location":"odkconflation/#odk-collect-central","title":"ODK Collect &amp; Central","text":"<p>ODK Collect is a mobile app for data collection using XLSForms. It's server side is ODK Central, which replaces the  older ODK Aggregate. ODK Central manages the XLSForms downloaded to your phone, as wall as the submissions uploaded from your phone when back online.</p> <p>A related project for processing ODK data and working remotely with Central is osm-fieldwork. This Python project handles conversion of the various data files from Collect or Central, into OSM XML and GeoJson for future processing via editing or conflation. This is heavily used in the FMTM backend.</p>"},{"location":"odkconflation/#field-data-collection","title":"Field Data Collection","text":"<p>Collecting data in the field is to best way to add data to OpenStreetMap. Whether done by casual mappers adding POIs, to more dedicated mappers, what is reality at that moment is the key to keeping OSM fresh and updated. When it comes to improving the metadata for buildings, many have been imported with building=yes from remote mapping using the HOT Tasking Manager to trace buildings from satellite imagery. </p> <p>But ground-truthing what kind of building it is improvers the map. It may be a medical clinic, restaurant, residence, etc.. who know until somebody stands in front of the building to collect more informsation about it. This may be idenifying it as a clinic or reseidence, adding the building material, what is the roof made of, is it's power non-existance, or are there solar panels or a generator ? Some humanitarian mapping is collecting data on public toilets, and community water sources for future improvements. </p> <p>Knowing there is a building on the map is useful, but better yet is what is the building used for ? What is it made of ? Does it have AC or DC power ? Water available ? All of these details improve the map to make it more useful to others.</p>"},{"location":"odkconflation/#field-mapping-camping-manager","title":"Field Mapping Camping Manager","text":"<p>The Field Mapping Camping Manager (FMTM) is a project to oprganize large scale data collection using ODK Collect and ODK Central. It uses the osm-fieldwork project for much of the backend processing of the ODK data,  but is designed for large scale field mapping involving many people. It uses ODK Collect and ODK Central as the primary tools. One of the final steps in processing ODK data to import into OSM is conflating it with existing data. This can be done manually of course, but with a large number of data submissions this becomes tedious and time consuming. FMTM aggrgates all the data for an entire project, and may have thousands of submissions. This is where conflation is critical.</p>"},{"location":"odkconflation/#the-algorythm","title":"The Algorythm","text":"<p>Currently conflation is focused on ODK with OSM. This uses the conflator.py program which can conflate between the ODK data and an OSM data extract. There are other conflation programs in this project for other external datasets, but uses a postgres database instead of two files.</p>"},{"location":"odkconflation/#the-conflator-class","title":"The Conflator() Class","text":"<p>This is the primary interface for conflating files. It has two primary endpoint. This top level endpoint is Conflator.conflateFiles(), which is used when the conflator program is run standalone. It opens the two disk files, parses the various formats, and generates a data structure used for conflation. This class uses the Parsers() class from osm-fieldwork that can parse the JSON or CSV files downloaded from ODK Central, or the ODK XML \"instance\" files when working offline. OPSM XML or GeoJson files are also supported. Each entry in the files is turned into list of python dicts to make it easier to compaert the data.</p> <p>Once the two files are read, the Conflator.conflateFeatures() endpoint takes the two lists of data and does the actual conflation. There is an additional parameter passed to this endpoint that is the threshold distance. This is used to find all features in the OSM data extract within that distance. Note that this is a unit of the earth's circumforance, not meters, so distance calulations are a bit fuzzy.</p> <p>This is a brute force conflation algorythm, not fast but it tries to be complete. it is comprised of two loops. The top level loops through the ODK data. For each ODK data entry, it finds all the OSM features within that threshold distance. The inner loop then uses the closest feature and compares the tags. This is where things get interesting.... If there is a name tag in the ODK data, this is string compared with the name in the closest OSM feature. Fuzzy string matching is used to handle minor spelling differences. Sometimes the mis-spelling is in the OSM data, but often when entering names of features on your smartphone, mis-typing occurs. If there is a 100% match in the name tags, then chances are the feature exists in OSM already.</p> <p>If there is no name tag in the ODK data, then the other tags are compared to try to find a possible duplicate feature. For example, a public toilet at a trailhead has no name, but if both ODK and OSM have amenity=toilet, then it's very likey a duplicate. If no tags match, then the ODK data is proably a new feature.</p> <p>Any time a possible duplicate is found, it is not automatically merged. Instead a fixme tag is added to the feature in the output file with a statement that it is potentially a duplicate. When the output file is loaded into JOSM, you can search for this tag to manually decide if it is a duplicate.</p>"},{"location":"odkconflation/#xlsform-design","title":"XLSForm Design","text":"<p>Part of the key detail to improve conflation requires a carefully created XLSForm. There is much more detailed information on XLSForm design, but briefly whatever is in the name column in the survey sheet becomes the name of the tags, and whatever is in the name column in the choices sheet becomes the value. If you want a relatively smooth conflation, make sure your XLSForm uses OSM tagging schemas.</p> <p>If you don't follow OSM tagging, then conflation will assumme all your ODK data is a new feature, and you'll have to manually conflate the results using JOSM. That's OK for small datasets, but quickly becomes very tedious for the larger datasets that FMTM collects.</p>"},{"location":"odkconflation/#the-output-file","title":"The Output File","text":"<p>The output file must be in OSM XML to enable updating the ways. If the OSM data is a POI, viewing it in JOSM is easy. If the OSM data is a polygon, when loaded into JOSM, they won't appear at first. Since the OSM way created by conflation has preserved the refs used by OSM XML to reference the nodes, doing update modified in JOSM then pulls down the nodes and all the polygons will appear.</p>"},{"location":"odkconflation/#conflicts","title":"Conflicts","text":"<p>There are some interesting issues to fix post conflation. ODK data is usually a single POI, whereas in OSM it may be a polygon. Sometimes though the POI is already in OSM. Remote mapping or building footprint imports often have a polygon with a single building=yes tag. If the POI we collected in ODK has more data, for example this building is a restaurant serving pizza, and is made of brick.</p> <p>In OSM sometimes there is a POI for an amenity, as well as a building polygon that were added at different times by different people.  The key detail for conflation is do any of the tags and values from the new data match existing data ?</p> <p>FMTM downloads a data extract from OSM using osm-rawdata, and then filters the data extract based on what is on the choices sheet of the XLSForm. Otherwise Collect won't launch. Because this data extract does not contain all the tags that are in OSM, it creates conflicts. This problem is FMTM specific, and can be improved by making more complete data extract from OSM.</p> <p>When the only tag in the OSM data is building=, any tags from ODK are merged with the building polygon when possible. If the OSM feature has other tags, JOSM will flag this as a conflict. Then you have to manually merge the tags in JOSM.</p>"},{"location":"osm-merge/","title":"Conflator Program","text":"<p>osm-merge is a program that conflates building footprint data with OpenStreetMap data to remove duplicates. The result of the conflation process is buildings that only exist in the footprints data file.</p> <p>This program can process data from either a postgres database, or data files in geojson, shapefile format. One of the core concepts is using a data file of polygons to filter the larger datasets, since a database may contain multiple countries.</p> <p>The process of setting up for large scale conflation is in this document.</p>"},{"location":"osm-merge/#command-line-options","title":"Command Line Options","text":""},{"location":"osm-merge/#common-options","title":"Common Options","text":"<p>These are the nost commonly used options.</p> <pre><code>--help(-h)       Get command line options\n--verbose(-v)    Enable verbose output\n--boundary(-b)   Specify a multipolygon for boundaries, one file for each polygon\n--project(-p)    Tasking Manager project ID to get boundaries from database\n--osmdata(-x)    OSM XML/PBF or OSM database to get boundaries (prefix with pg: if database)\n--outdir(-o)     Output file prefix for output files (default \"/tmp/tmproject-\")\n--footprints(-f) File or building footprints Database URL (prefix with pg: if database)\n--dbhost(-d)     Database host, defaults to \"localhost\"\n--dbuser(-u)     Database user, defaults to current user\n--dbpass(-w)     Database user, defaults to no password needed\n</code></pre>"},{"location":"osm-merge/#tasking-manager-options","title":"Tasking Manager Options","text":"<p>These options are used to dynamically extract a project boundary from a Tasking Manager database. A more common usage is to use the splitter.py program to download the project boundary from the Tasking Manager itself.</p> <pre><code>--splittasks     When using the Tasking Manager database, split into tasks\n--schema         OSM database schema (pgsnapshot, ogr2ogr, osm2pgsql) defaults to \"pgsnapshot\"\n--tmdata(-t)     Tasking Manager database to get boundaries if no boundary file prefix with pg: for database usage, http for REST API\n</code></pre>"},{"location":"osm-merge/#osm-options","title":"OSM Options","text":"<p>When extracting administrative boundaries from an OpenStreetMap database, the default admin levl is 4, which is commonly used for couty boundaries. This lets the user select what level of administrative boundaries they want.</p> <pre><code>--admin(-a)      When querying the OSM database, this is the admin_level, (defaults to 4)\n</code></pre>"},{"location":"osm-merge/#examples","title":"Examples","text":"<p>PATH/conflator.py -v -x 12057-osm.geojson -f 12057-ms.geojson -o 12057</p> <p>This takes two disk files, which have already been filtered to only contain data for the area to conflate.</p> <p>PATH/conflator.py -v -x pg:kenya -b 12007-project.geojson -f 12057-ms.geojson -o 12057</p> <p>This uses a database that contains all of Kenya, but we only want to process a single project, so that's supplied as the boundary. The foorptin data was already filtered using ogr2ogr, and the project ID is used as the prefix for the output files.</p> <p>PATH/conflator.py -v -x pg:kenya -b 12007-project.geojson -f pg:kenya_footprints -o 12057 -d mapdb -u me</p> <p>This is the same except the database is on a remote machine called mapdb and the user needs to be me.</p> <p>PATH/conflator.py -t tmsnap -p 8345 -b pg:kenya_foot -o pg:Kenya</p> <p>Reads from 3 data sources. The first one is a snapshot of the Tasking Manager database, and we want to use project 8345 as the boundary. The two data sources are prefixed with \"pg\", which defines them as a database URL instead of a file. The database needs to be running locally in this case.</p>"},{"location":"results/","title":"Conflation Results In Kenya","text":""},{"location":"results/#project-12007","title":"Project 12007","text":"<p>Good quality conflation over all. There seems to be a parking lot of semi-trucks misidentifed as buildings. Another is a pond identified as a building.</p>"},{"location":"results/#90-new-buildings","title":"90 new buildings","text":""},{"location":"results/#project-12012","title":"Project 12012","text":"<p>Good quality conflation over all. Some fenced yards are being identified as a building.</p>"},{"location":"results/#374-new-buildings","title":"374 new buildings","text":""},{"location":"results/#project-12013","title":"Project 12013","text":"<p>Good quality conflation over all. In dense area there is enough space between buildings.</p>"},{"location":"results/#177-new-buildings","title":"177 new buildings","text":""},{"location":"results/#project-12057","title":"Project 12057","text":""},{"location":"results/#project-12091","title":"Project 12091","text":""},{"location":"results/#project-12092","title":"Project 12092","text":"<p>FIXME: Already imported, need new data file</p>"},{"location":"results/#project-12093","title":"Project 12093","text":"<p>An entire village was missing, so lots of new buildings. Any buildings added to OSM after the file at Geofabrik was created.</p>"},{"location":"results/#34976-new-buildings","title":"34976 new buildings","text":""},{"location":"results/#project-12340","title":"Project 12340","text":""},{"location":"results/#project-12351","title":"Project 12351","text":"<p>A round feature is misidentified as a square building. Any buildings added to OSM after the file at Geofabrik was created. An entire village was missing, so lots of new buildings.</p>"},{"location":"results/#4566-new-buildings","title":"4566 new buildings","text":""},{"location":"utilities/","title":"Utility Programs","text":"<p>To conflate external datasets with OSM, the external data needs to be converted to the OSM tagging schema. Otherwise comparing tags gets very convoluted. Since every dataset uses a different schema, included are a few utility programs for converting external datasets. Currently the only datatsets are for highways. These datasets are available from the USDA, and have an appropriate license to use with OpenStreetMap. Indeed, some of this data has already been imported. The files are available from the  FSGeodata Clearinghouse</p> <p>Most of the fields in the dataset aren't needed for OSM, only the reference number if it has one, and the name. Most of these highways are already in OSM, but it's a bit of a mess, and mostly unvalidated. Most of the problems are related to the TIGER import in 2007. So the goal of these utilities is to add in the TIGER fixup work by updating or adding the name and a reference number. These utilities prepare the dataset for conflation.</p> <p>There are other fields in the datasets we might want, like surface type, is it 4wd only, etc... but often the OSM data is more up to date. And to really get that right, you need to ground truth it.</p>"},{"location":"utilities/#mvumpy","title":"mvum.py","text":"<p>This converts the Motor Vehicle Use Map(MVUM) dataset that contains data on highways more suitable for offroad vehicles. Some require specialized offroad vehicles like a UTV or ATV. The data in OSM for these roads is really poor. Often the reference number is wrong, or lacks the suffix. We assume the USDA data is correct when it comes to name and reference number, and this will get handled later by conflation.</p>"},{"location":"utilities/#roadcorepy","title":"roadcore.py","text":"<p>This converts the Road Core vehicle map. This contains data on all highways in a national forest. It's similar to the MVUM dataset.</p>"},{"location":"utilities/#trailspy","title":"Trails.py","text":"<p>This converts the NPSPublish Trail dataset. These are hiking trails not open to motor vehicles. Currently much of this dataset has empty fields, but the trail name and reference number is useful. This utility is to support the OpenStreetMap US Trails Initiative.</p>"},{"location":"wiki_redirect/","title":"OSM RawData","text":"<p>Please see the docs page at: https://hotosm.github.io/conflator/</p>"}]}